{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distilbert",
      "provenance": [],
      "collapsed_sections": [
        "sJsBpETWrSlk"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDR80nrpYJSZ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJsBpETWrSlk"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5DJt9KGoSf5"
      },
      "source": [
        "!git clone https://github.com/neuspell/neuspell\n",
        "%cd neuspell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiymHeLeoZfU"
      },
      "source": [
        "!pip install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LolSwfmocil"
      },
      "source": [
        "!pip install urllib3==1.25.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0owUYWg2pKTQ"
      },
      "source": [
        "!pip install folium==0.2.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBqDR09Konge"
      },
      "source": [
        "!pip install -r extras-requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFTbuKdqos-z"
      },
      "source": [
        "!pip install torch==1.6.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se7YcT-Tphfd"
      },
      "source": [
        "!pip install transformers==4.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaaDd1X7DZll"
      },
      "source": [
        "import neuspell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r16B7f6EqCzu"
      },
      "source": [
        "%cd data/traintest\n",
        "!python download_datafiles.py "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWAH8SYjqWqt"
      },
      "source": [
        "%cd /content/neuspell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIX83hQaU_4S"
      },
      "source": [
        "# Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZMoUfq1xOso"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "\n",
        "from neuspell.seq_modeling.subwordbert import load_model\n",
        "from neuspell.seq_modeling.helpers import load_data, train_validation_split, batch_accuracy_func\n",
        "from neuspell.seq_modeling.helpers import get_tokens, progressBar\n",
        "from neuspell.seq_modeling.helpers import batch_iter, labelize, tokenize, bert_tokenize_for_valid_examples\n",
        "\n",
        "from neuspell.seq_modeling.helpers import load_vocab_dict, save_vocab_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsq2r8BYxQRd"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TKBAuP2_7KL"
      },
      "source": [
        "## Load dataset, vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1vujZqTxWBj"
      },
      "source": [
        "train_data = load_data('/content/neuspell/data/traintest/','test.1blm','test.1blm.noise.prob')\n",
        "\n",
        "train_data, valid_data = train_validation_split(train_data, 0.90, seed=1)\n",
        "\n",
        "vocab_ref = {}\n",
        "\n",
        "vocab = get_tokens([i[0] for i in train_data],\n",
        "                           keep_simple=True,\n",
        "                           min_max_freq=(2,float(\"inf\")),\n",
        "                           topk=100000,\n",
        "                           intersect=vocab_ref,\n",
        "                           load_char_tokens=True)\n",
        "\n",
        "# save_vocab_dict('/content/drive/MyDrive/NLP/bert_vocab.pkl', vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-Y6NsXyADRe"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOxXin23x1Q1"
      },
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "bert_pretrained_name_or_path = \"distilbert-base-cased\"\n",
        "model = load_model(vocab,\"distilbert-base-cased\")\n",
        "model = model.cuda()\n",
        "\n",
        "\n",
        "VALID_BATCH_SIZE = 32\n",
        "\n",
        "data_iter = batch_iter(train_data, batch_size=VALID_BATCH_SIZE, shuffle=False)\n",
        "\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "\n",
        "DEVICE = 'cuda'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiOvZITX_wpf"
      },
      "source": [
        "## freeze layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKOHTfURHwZH"
      },
      "source": [
        "# for layers in model.bert_model.encoder.layer[:9]:\n",
        "#     for param in layers.parameters():\n",
        "#         param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-NZQEzCAFiq"
      },
      "source": [
        "## Bert optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCxdnfu1AJQw"
      },
      "source": [
        "START_EPOCH = 1\n",
        "N_EPOCHS = 5\n",
        "\n",
        "GRADIENT_ACC = 4\n",
        "max_dev_acc, argmax_dev_acc = -1, -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_2wJjsRKvZH"
      },
      "source": [
        "# from pytorch_pretrained_bert import BertAdam\n",
        "\n",
        "# param_optimizer = list(model.named_parameters())\n",
        "# no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "# optimizer_grouped_parameters = [\n",
        "#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "# ]\n",
        "# t_total = int(len(train_data) / TRAIN_BATCH_SIZE / GRADIENT_ACC * N_EPOCHS)\n",
        "# optimizer = BertAdam(optimizer_grouped_parameters,lr=5e-5,warmup=0.1,t_total=t_total)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJdS9f_0AJyZ"
      },
      "source": [
        "## Set epoch, start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK4WyCzJLbfl"
      },
      "source": [
        "BERT_TOKENIZER = None\n",
        "import transformers\n",
        "from typing import List\n",
        "BERT_MAX_SEQ_LEN = 512\n",
        "\n",
        "def merge_subtokens(tokens: List):\n",
        "    merged_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.startswith(\"##\"):\n",
        "            merged_tokens[-1] = merged_tokens[-1] + token[2:]\n",
        "        else:\n",
        "            merged_tokens.append(token)\n",
        "    text = \" \".join(merged_tokens)\n",
        "    return text\n",
        "\n",
        "def _custom_bert_tokenize_sentence(text):\n",
        "    tokens = BERT_TOKENIZER.tokenize(text)\n",
        "    tokens = tokens[:BERT_MAX_SEQ_LEN - 2]  # 2 allowed for [CLS] and [SEP]\n",
        "    idxs = np.array([idx for idx, token in enumerate(tokens) if not token.startswith(\"##\")] + [len(tokens)])\n",
        "    split_sizes = (idxs[1:] - idxs[0:-1]).tolist()\n",
        "    # NOTE: BERT tokenizer does more than just splitting at whitespace and tokenizing. So be careful.\n",
        "    # -----> assert len(split_sizes)==len(text.split()), print(len(tokens), len(split_sizes), len(text.split()), split_sizes, text)\n",
        "    # -----> hence do the following:\n",
        "    text = merge_subtokens(tokens)\n",
        "    assert len(split_sizes) == len(text.split()), print(len(tokens), len(split_sizes), len(text.split()), split_sizes,\n",
        "                                                        text)\n",
        "    return text, tokens, split_sizes\n",
        "\n",
        "def _custom_bert_tokenize_sentences(list_of_texts):\n",
        "    out = [_custom_bert_tokenize_sentence(text) for text in list_of_texts]\n",
        "    texts, tokens, split_sizes = list(zip(*out))\n",
        "    return [*texts], [*tokens], [*split_sizes]\n",
        "\n",
        "def _simple_bert_tokenize_sentences(list_of_texts):\n",
        "    return [merge_subtokens(BERT_TOKENIZER.tokenize(text)[:BERT_MAX_SEQ_LEN - 2]) for text in list_of_texts]\n",
        "\n",
        "def bert_tokenize_for_valid_examples(batch_orginal_sentences, batch_noisy_sentences, bert_pretrained_name_or_path=bert_pretrained_name_or_path):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "        batch_noisy_sentences: List[str]\n",
        "            a list of textual sentences to tokenized\n",
        "        batch_orginal_sentences: List[str]\n",
        "            a list of texts to make sure lengths of input and output are same in the seq-modeling task\n",
        "        bert_pretrained_name_or_path:\n",
        "            a huggingface path for loading a custom bert model\n",
        "    outputs (only of batch_noisy_sentences):\n",
        "        batch_attention_masks, batch_input_ids, batch_token_type_ids\n",
        "            2d tensors of shape (bs,max_len)\n",
        "        batch_splits: List[List[Int]]\n",
        "            specifies #sub-tokens for each word in each textual string after sub-word tokenization\n",
        "    \"\"\"\n",
        "    global BERT_TOKENIZER\n",
        "\n",
        "    if BERT_TOKENIZER is None:  # gets initialized during the first call to this method\n",
        "        if bert_pretrained_name_or_path:\n",
        "            BERT_TOKENIZER = transformers.BertTokenizer.from_pretrained(bert_pretrained_name_or_path)\n",
        "            BERT_TOKENIZER.do_basic_tokenize = True\n",
        "            BERT_TOKENIZER.tokenize_chinese_chars = False\n",
        "        else:\n",
        "            BERT_TOKENIZER = transformers.BertTokenizer.from_pretrained(bert_pretrained_name_or_path)\n",
        "            BERT_TOKENIZER.do_basic_tokenize = True\n",
        "            BERT_TOKENIZER.tokenize_chinese_chars = False\n",
        "\n",
        "    _batch_orginal_sentences = _simple_bert_tokenize_sentences(batch_orginal_sentences)\n",
        "    _batch_noisy_sentences, _batch_tokens, _batch_splits = _custom_bert_tokenize_sentences(batch_noisy_sentences)\n",
        "\n",
        "    valid_idxs = [idx for idx, (a, b) in enumerate(zip(_batch_orginal_sentences, _batch_noisy_sentences)) if\n",
        "                  len(a.split()) == len(b.split())]\n",
        "    batch_orginal_sentences = [line for idx, line in enumerate(_batch_orginal_sentences) if idx in valid_idxs]\n",
        "    batch_noisy_sentences = [line for idx, line in enumerate(_batch_noisy_sentences) if idx in valid_idxs]\n",
        "    batch_tokens = [line for idx, line in enumerate(_batch_tokens) if idx in valid_idxs]\n",
        "    batch_splits = [line for idx, line in enumerate(_batch_splits) if idx in valid_idxs]\n",
        "\n",
        "    batch_bert_dict = {\n",
        "        \"attention_mask\": [],\n",
        "        \"input_ids\": [],\n",
        "        # \"token_type_ids\": []\n",
        "    }\n",
        "    if len(valid_idxs) > 0:\n",
        "        batch_encoded_dicts = [BERT_TOKENIZER.encode_plus(tokens) for tokens in batch_tokens]\n",
        "        batch_attention_masks = pad_sequence(\n",
        "            [torch.tensor(encoded_dict[\"attention_mask\"]) for encoded_dict in batch_encoded_dicts], batch_first=True,\n",
        "            padding_value=0)\n",
        "        batch_input_ids = pad_sequence(\n",
        "            [torch.tensor(encoded_dict[\"input_ids\"]) for encoded_dict in batch_encoded_dicts], batch_first=True,\n",
        "            padding_value=0)\n",
        "        # batch_token_type_ids = pad_sequence(\n",
        "        #     [torch.tensor(encoded_dict[\"token_type_ids\"]) for encoded_dict in batch_encoded_dicts], batch_first=True,\n",
        "        #     padding_value=0)\n",
        "        batch_bert_dict = {\"attention_mask\": batch_attention_masks,\n",
        "                           \"input_ids\": batch_input_ids,\n",
        "                           # \"token_type_ids\": batch_token_type_ids\n",
        "                           }\n",
        "\n",
        "    return batch_orginal_sentences, batch_noisy_sentences, batch_bert_dict, batch_splits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqh28V51yCxO"
      },
      "source": [
        "# train and eval\n",
        "for epoch_id in range(START_EPOCH,N_EPOCHS+1):\n",
        "\n",
        "    print(f\"In epoch: {epoch_id}\")\n",
        "\n",
        "    # train loss and backprop\n",
        "    train_loss = 0.\n",
        "    train_acc = 0.\n",
        "    train_acc_count = 0.\n",
        "    print(\"train_data size: {}\".format(len(train_data)))\n",
        "    \n",
        "    train_data_iter = batch_iter(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "    nbatches = int(np.ceil(len(train_data)/TRAIN_BATCH_SIZE))\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for batch_id, (batch_labels,batch_sentences) in enumerate(train_data_iter):\n",
        "        optimizer.zero_grad()\n",
        "        st_time = time.time()\n",
        "\n",
        "        # set batch data for bert\n",
        "        batch_labels_, batch_sentences_, batch_bert_inp, batch_bert_splits = bert_tokenize_for_valid_examples(batch_labels,batch_sentences,bert_pretrained_name_or_path=bert_pretrained_name_or_path)                \n",
        "        if len(batch_labels_)==0:\n",
        "            print(\"################\")\n",
        "            print(\"Not training the following lines due to pre-processing mismatch: \\n\")\n",
        "            print([(a,b) for a,b in zip(batch_labels,batch_sentences)])\n",
        "            print(\"################\")\n",
        "            continue\n",
        "        else:\n",
        "            batch_labels, batch_sentences = batch_labels_, batch_sentences_\n",
        "\n",
        "        batch_bert_inp = {k:v.to(DEVICE) for k,v in batch_bert_inp.items()}\n",
        "\n",
        "        # set batch data for others\n",
        "        batch_labels, batch_lengths = labelize(batch_labels, vocab)\n",
        "        batch_lengths = batch_lengths.to(DEVICE)\n",
        "        batch_labels = batch_labels.to(DEVICE)\n",
        "\n",
        "        # forward\n",
        "        model.train()\n",
        "        \n",
        "        loss = model(batch_bert_inp, batch_bert_splits, targets=batch_labels)\n",
        "        \n",
        "        batch_loss = loss.cpu().detach().numpy()\n",
        "        train_loss += batch_loss\n",
        "\n",
        "        # backward\n",
        "        # if GRADIENT_ACC > 1:\n",
        "        #     loss = loss / GRADIENT_ACC\n",
        "        loss.backward()\n",
        "        # step\n",
        "        # if (batch_id + 1) % GRADIENT_ACC == 0 or batch_id >= nbatches - 1:\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "            # scheduler.step()\n",
        "        # optimizer.zero_grad()\n",
        "\n",
        "        # compute accuracy in numpy\n",
        "        if batch_id%10000==0:\n",
        "\n",
        "            train_acc_count += 1\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                _, batch_predictions = model(batch_bert_inp, batch_bert_splits, targets=batch_labels)\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            batch_labels = batch_labels.cpu().detach().numpy()\n",
        "            batch_lengths = batch_lengths.cpu().detach().numpy()\n",
        "            ncorr,ntotal = batch_accuracy_func(batch_predictions,batch_labels,batch_lengths)\n",
        "            batch_acc = ncorr/ntotal\n",
        "            train_acc += batch_acc     \n",
        "\n",
        "        # update progress\n",
        "        progressBar(batch_id+1,\n",
        "                    int(np.ceil(len(train_data) / TRAIN_BATCH_SIZE)), \n",
        "                    [\"batch_time\",\"batch_loss\",\"avg_batch_loss\",\"batch_acc\",\"avg_batch_acc\"],\n",
        "                    [time.time()-st_time,batch_loss,train_loss/(batch_id+1),batch_acc,train_acc/train_acc_count]) \n",
        "    \n",
        "    print(f\"\\nEpoch {epoch_id} train_loss: {train_loss/(batch_id+1)}\")\n",
        "\n",
        "    # save model every epoch\n",
        "    model_name = \"bert_epoch_\" + str(epoch_id) + '.pt'\n",
        "    # torch.save(model.state_dict(), \n",
        "    #         '/content/drive/MyDrive/NLP/'+model_name)\n",
        "\n",
        "    # valid loss\n",
        "    valid_loss = 0.\n",
        "    valid_acc = 0.\n",
        "    print(\"valid_data size: {}\".format(len(valid_data)))\n",
        "\n",
        "    valid_data_iter = batch_iter(valid_data, batch_size=VALID_BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    for batch_id, (batch_labels,batch_sentences) in enumerate(valid_data_iter):\n",
        "\n",
        "        st_time = time.time()\n",
        "        # set batch data for bert\n",
        "        # batch_labels_, batch_sentences_, batch_bert_inp, batch_bert_splits = bert_tokenize_for_valid_examples(batch_labels,batch_sentences)\n",
        "\n",
        "        batch_labels, batch_sentences, batch_bert_inp, batch_bert_splits = bert_tokenize_for_valid_examples(batch_labels,batch_sentences)\n",
        "        \"\"\"\n",
        "        if len(batch_labels_)==0:\n",
        "            print(\"################\")\n",
        "            print(\"Not validating the following lines due to pre-processing mismatch: \\n\")\n",
        "            print([(a,b) for a,b in zip(batch_labels,batch_sentences)])\n",
        "            print(\"################\")\n",
        "            continue\n",
        "        else:\n",
        "        \n",
        "            batch_labels, batch_sentences = batch_labels_, batch_sentences_\n",
        "        \"\"\"\n",
        "\n",
        "        batch_bert_inp = {k:v.to(DEVICE) for k,v in batch_bert_inp.items()}\n",
        "\n",
        "\n",
        "        # set batch data for others\n",
        "        batch_labels, batch_lengths = labelize(batch_labels, vocab)\n",
        "        batch_lengths = batch_lengths.to(DEVICE)\n",
        "        batch_labels = batch_labels.to(DEVICE)\n",
        "\n",
        "        # forward\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            batch_loss, batch_predictions = model(batch_bert_inp, batch_bert_splits, targets=batch_labels)\n",
        "        model.train()        \n",
        "        valid_loss += batch_loss\n",
        "        # compute accuracy in numpy\n",
        "        batch_labels = batch_labels.cpu().detach().numpy()\n",
        "        batch_lengths = batch_lengths.cpu().detach().numpy()\n",
        "        ncorr,ntotal = batch_accuracy_func(batch_predictions,batch_labels,batch_lengths)\n",
        "        batch_acc = ncorr/ntotal\n",
        "        valid_acc += batch_acc\n",
        "        # update progress\n",
        "        progressBar(batch_id+1,\n",
        "                    int(np.ceil(len(valid_data) / VALID_BATCH_SIZE)), \n",
        "                    [\"batch_time\",\"batch_loss\",\"avg_batch_loss\",\"batch_acc\",\"avg_batch_acc\"], \n",
        "                    [time.time()-st_time,batch_loss,valid_loss/(batch_id+1),batch_acc,valid_acc/(batch_id+1)])\n",
        "\n",
        "    print(f\"\\nEpoch {epoch_id} valid_loss: {valid_loss/(batch_id+1)}\")\n",
        "    torch.save({\n",
        "            'epoch': epoch_id,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': valid_loss}, f'/content/gdrive/MyDrive/mdistilepoch:{epoch_id}valid_acc{valid_acc/(batch_id+1)}.hdf5')\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUbevWLeAUvb"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-zb8Iug02Ba"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from neuspell.seq_modeling.evals import get_metrics\n",
        "\n",
        "def untokenize_without_unks(batch_predictions, batch_lengths, vocab, batch_clean_sentences, backoff=\"pass-through\"):\n",
        "    assert backoff in [\"neutral\", \"pass-through\"], print(f\"selected backoff strategy not implemented: {backoff}\")\n",
        "    idx2token = vocab[\"idx2token\"]\n",
        "    unktoken = vocab[\"token2idx\"][vocab[\"unk_token\"]]\n",
        "    assert len(batch_predictions) == len(batch_lengths) == len(batch_clean_sentences)\n",
        "    batch_clean_sentences = [sent.split() for sent in batch_clean_sentences]\n",
        "    if backoff == \"pass-through\":\n",
        "        batch_predictions = \\\n",
        "            [\" \".join([idx2token[idx] if idx != unktoken else clean_[i] for i, idx in enumerate(pred_[:len_])]) \\\n",
        "             for pred_, len_, clean_ in zip(batch_predictions, batch_lengths, batch_clean_sentences)]\n",
        "    elif backoff == \"neutral\":\n",
        "        batch_predictions = \\\n",
        "            [\" \".join([idx2token[idx] if idx != unktoken else \"a\" for i, idx in enumerate(pred_[:len_])]) \\\n",
        "             for pred_, len_, clean_ in zip(batch_predictions, batch_lengths, batch_clean_sentences)]\n",
        "    return batch_predictions\n",
        "\n",
        "def batch_iter(data, batch_size, shuffle):\n",
        "    \"\"\"\n",
        "    each data item is a tuple of lables and text\n",
        "    \"\"\"\n",
        "    n_batches = int(np.ceil(len(data) / batch_size))\n",
        "    indices = list(range(len(data)))\n",
        "    if shuffle:  np.random.shuffle(indices)\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        batch_indices = indices[i * batch_size: (i + 1) * batch_size]\n",
        "        batch_labels = [data[idx][0] for idx in batch_indices]\n",
        "        batch_sentences = [data[idx][1] for idx in batch_indices]\n",
        "\n",
        "        yield (batch_labels, batch_sentences)\n",
        "\n",
        "def model_inference(model, data, topk, device, batch_size=16, vocab_=None):\n",
        "    \"\"\"\n",
        "    model: an instance of SubwordBert\n",
        "    data: list of tuples, with each tuple consisting of correct and incorrect \n",
        "            sentence string (would be split at whitespaces)\n",
        "    topk: how many of the topk softmax predictions are considered for metrics calculations\n",
        "    \"\"\"\n",
        "    if vocab_ is not None:\n",
        "        vocab = vocab_\n",
        "    print(\"###############################################\")\n",
        "    inference_st_time = time.time()\n",
        "    _corr2corr, _corr2incorr, _incorr2corr, _incorr2incorr = 0, 0, 0, 0\n",
        "    _mistakes = []\n",
        "    VALID_batch_size = batch_size\n",
        "    valid_loss = 0.\n",
        "    valid_acc = 0.\n",
        "    print(\"data size: {}\".format(len(data)))\n",
        "    data_iter = batch_iter(data, batch_size=VALID_batch_size, shuffle=False)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    for batch_id, (batch_labels, batch_sentences) in tqdm(enumerate(data_iter)):\n",
        "        torch.cuda.empty_cache()\n",
        "        st_time = time.time()\n",
        "        # set batch data for bert\n",
        "        batch_labels_, batch_sentences_, batch_bert_inp, batch_bert_splits = bert_tokenize_for_valid_examples(\n",
        "            batch_labels, batch_sentences, bert_pretrained_name_or_path=bert_pretrained_name_or_path)\n",
        "        if len(batch_labels_) == 0:\n",
        "            print(\"################\")\n",
        "            print(\"Not predicting the following lines due to pre-processing mismatch: \\n\")\n",
        "            print([(a, b) for a, b in zip(batch_labels, batch_sentences)])\n",
        "            print(\"################\")\n",
        "            continue\n",
        "        else:\n",
        "            batch_labels, batch_sentences = batch_labels_, batch_sentences_\n",
        "        batch_bert_inp = {k: v.to(device) for k, v in batch_bert_inp.items()}\n",
        "        # set batch data for others\n",
        "        batch_labels_ids, batch_lengths = labelize(batch_labels, vocab)\n",
        "        # batch_lengths = batch_lengths.to(device)\n",
        "        batch_labels_ids = batch_labels_ids.to(device)\n",
        "        # forward\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                \"\"\"\n",
        "                NEW: batch_predictions can now be of shape (batch_size,batch_max_seq_len,topk) if topk>1, else (batch_size,batch_max_seq_len)\n",
        "                \"\"\"\n",
        "                batch_loss, batch_predictions = model(batch_bert_inp, batch_bert_splits, targets=batch_labels_ids,\n",
        "                                                      topk=topk)\n",
        "                print(batch_predictions)\n",
        "        except RuntimeError:\n",
        "            print(f\"batch_bert_inp:{len(batch_bert_inp.keys())},batch_labels_ids:{batch_labels_ids.shape}\")\n",
        "            raise Exception(\"\")\n",
        "        valid_loss += batch_loss\n",
        "        # compute accuracy in numpy\n",
        "        batch_labels_ids = batch_labels_ids.cpu().detach().numpy()\n",
        "        batch_lengths = batch_lengths.cpu().detach().numpy()\n",
        "        # based on topk, obtain either strings of batch_predictions or list of tokens\n",
        "        if topk == 1:\n",
        "            batch_predictions = untokenize_without_unks(batch_predictions, batch_lengths, vocab, batch_sentences)\n",
        "        else:\n",
        "            batch_predictions = untokenize_without_unks2(batch_predictions, batch_lengths, vocab, batch_sentences,\n",
        "                                                         topk=None)\n",
        "        # corr2corr, corr2incorr, incorr2corr, incorr2incorr, mistakes = \\\n",
        "        #    get_metrics(batch_labels,batch_sentences,batch_predictions,check_until_topk=topk,return_mistakes=True)\n",
        "        # _mistakes.extend(mistakes)\n",
        "        # batch_labels = [line.lower() for line in batch_labels]\n",
        "        # batch_sentences = [line.lower() for line in batch_sentences]\n",
        "        # batch_predictions = [line.lower() for line in batch_predictions]\n",
        "        print(batch_predictions)\n",
        "        corr2corr, corr2incorr, incorr2corr, incorr2incorr = \\\n",
        "            get_metrics(batch_labels, batch_sentences, batch_predictions, check_until_topk=topk, return_mistakes=False)\n",
        "        _corr2corr += corr2corr\n",
        "        _corr2incorr += corr2incorr\n",
        "        _incorr2corr += incorr2corr\n",
        "        _incorr2incorr += incorr2incorr\n",
        "\n",
        "        # delete\n",
        "        del batch_loss\n",
        "        del batch_predictions\n",
        "        del batch_labels, batch_lengths, batch_bert_inp\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        '''\n",
        "        # update progress\n",
        "        progressBar(batch_id+1,\n",
        "                    int(np.ceil(len(data) / VALID_batch_size)), \n",
        "                    [\"batch_time\",\"batch_loss\",\"avg_batch_loss\",\"batch_acc\",\"avg_batch_acc\"], \n",
        "                    [time.time()-st_time,batch_loss,valid_loss/(batch_id+1),None,None])\n",
        "        '''\n",
        "    print(f\"\\nEpoch {None} valid_loss: {valid_loss / (batch_id + 1)}\")\n",
        "    print(\"total inference time for this data is: {:4f} secs\".format(time.time() - inference_st_time))\n",
        "    print(\"###############################################\")\n",
        "    print(\"\")\n",
        "    # for mistake in _mistakes:\n",
        "    #    print(mistake)\n",
        "    print(\"\")\n",
        "    print(\"total token count: {}\".format(_corr2corr + _corr2incorr + _incorr2corr + _incorr2incorr))\n",
        "    print(\n",
        "        f\"_corr2corr:{_corr2corr}, _corr2incorr:{_corr2incorr}, _incorr2corr:{_incorr2corr}, _incorr2incorr:{_incorr2incorr}\")\n",
        "    print(f\"accuracy is {(_corr2corr + _incorr2corr) / (_corr2corr + _corr2incorr + _incorr2corr + _incorr2incorr)}\")\n",
        "    print(f\"word correction rate is {(_incorr2corr) / (_incorr2corr + _incorr2incorr)}\")\n",
        "    print(\"###############################################\")\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCtk0JhWyGW9"
      },
      "source": [
        "# from neuspell.seq_modeling.subwordbert import model_inference\n",
        "\n",
        "test_data = load_data('/content/neuspell/data/traintest/','test.bea60k','test.bea60k.noise')\n",
        "\n",
        "predicted_result = model_inference(model, test_data, 1, 'cuda', 16, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc33y9sNNUQk"
      },
      "source": [
        "\n",
        "test_data = load_data('/content/neuspell/data/traintest/','test.jfleg','test.jfleg.noise')\n",
        "\n",
        "predicted_result = model_inference(model, test_data, 1, 'cuda', 16, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}